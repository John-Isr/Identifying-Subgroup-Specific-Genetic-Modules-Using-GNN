# Identifying Subgroup-Specific Genetic Modules in Gene-Gene Correlation Networks Using Graph Neural Networks

## ğŸ“Œ Project Overview

#TODO: Update this section with an extensive overview of the process and results.

This project leverages Graph Neural Networks (GNNs) to **identify subgroup-specific clusters** within gene-gene correlation networks using a supervised learning approach. Each graph in the dataset contains exactly **one subgroup-specific cluster**, and our objective is to classify nodes as either **belonging (1)** or **not belonging (0)** to this cluster.

Key components include:

- **Graph Simulation & Preprocessing:** Generate raw graphs and initialize edge features.
- **GNN Architecture:** A custom classifier built with GNN layers (e.g., GATv2Conv) that incorporates edge attributes, attention mechanisms, and normalization (GraphNorm) to effectively learn node representations.
- **Training & Hyperparameter Tuning:** Training routines and hyperparameter optimization using Optuna with W&B logging.
- **Inference Pipeline:** A clean interface to convert NetworkX graphs to annotated graphs with node predictions.
- **Experiment Tracking:** Real-time logging and visualization with Weights & Biases (wandb).

---

## ğŸ“ Repository Structure


```
Identifying-Subgroup-Specific-Genetic-Modules-Using-GNN
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ graphs/                    # Raw generated graphs
â”‚   â””â”€â”€ modified_graphs/           # Graphs with initialized edge features
â”œâ”€â”€ data_pipeline/
â”‚   â”œâ”€â”€ __init__.py                # Module initializer for the data pipeline
â”‚   â”œâ”€â”€ graph_generator.py         # Graph simulation and generation
â”‚   â”œâ”€â”€ init_edge_features.py      # Edge feature initialization
â”‚   â””â”€â”€ simulation.py              # Graph simulation framework
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ optuna_studies/
â”‚   â”‚   â”œâ”€â”€ example_study.db       # Example database for Optuna studies
â”‚   â”‚   â””â”€â”€ study.db               # Optuna study results, will be created when hyperparameter tuning
â”‚   â”œâ”€â”€ trained_model.pt           # Default trained model weights
â”‚   â”œâ”€â”€ current_trial_best.yaml    # Best trial configuration from the currently running study
â”‚   â”œâ”€â”€ default_config.yaml        # Default hyper parameter configuration
â”‚   â””â”€â”€ trial_best.yaml            # Best trial configuration after hyperparameter tuning
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py                # Module initializer for models
â”‚   â”œâ”€â”€ architecture.py            # GNN architecture and weight initialization
â”‚   â”œâ”€â”€ hyperparameter_tuning.py   # Hyperparameter tuning routines (optuna objective function)
â”‚   â””â”€â”€ training.py                # Training routines for the model
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Project.ipynb              # Jupyter Notebook for project exploration (not pushed to main repository).
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ __init__.py                # Module initializer for scripts
â”‚   â”œâ”€â”€ inference.py               # Script for inference on graphs
â”‚   â”œâ”€â”€ run_pipeline.sh            # Shell script to run the entire data pipeline with default values
â”‚   â”œâ”€â”€ train_model.py             # Script to train the model using default or custom configuration
â”‚   â””â”€â”€ tune_hyperparams.py        # Script to tune model hyperparameters
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ edge_init_test.py          # Test for edge initialization
â”‚   â”œâ”€â”€ edge_init_test_load_parse.py  # Test for loading and parsing edge initialization
â”‚   â”œâ”€â”€ graph_generator_test.py    # Tests for graph generator
â”‚   â”œâ”€â”€ graph_generator_test2.py   # Additional tests for graph generator
â”‚   â””â”€â”€ test_graph_generator_cli.py # CLI tests for graph generator
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py                # Module initializer for utilities
â”‚   â”œâ”€â”€ logging.py                 # Logging utilities for W&B integration
â”‚   â””â”€â”€ preprocessing.py           # Preprocessing utilities
â”œâ”€â”€ .gitignore                     # Git ignore file
â”œâ”€â”€ LICENSE                        # License information
â”œâ”€â”€ README.md                      # Project documentation
â””â”€â”€ requirements.txt               # Python dependencies

```

---

## ğŸ”§ Setup Instructions

1. **Clone the Repository:**
   ```bash
   git clone https://github.com/John-Isr/Identifying-Subgroup-Specific-Genetic-Modules-Using-GNN.git
   cd Identifying-Subgroup-Specific-Genetic-Modules
   ```

2. **Create and Activate a Virtual Environment:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install Dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set Up Weights & Biases (wandb):**
   ```bash
   wandb login
   ```
   *(Obtain your API key by creating a free account at [wandb.ai](https://wandb.ai/).)*

---
## ğŸš€ Running the Project

### 1. Generate and Preprocess Data
Generate raw graphs and initialize their edge features:
```bash
  ./scripts/run_pipeline.sh
```

**Note:**
If you want to customize the graphs you're generating, you can use the following commands:
```bash
python data_pipeline/graph_generator.py --conditions Optimal Suboptimal Default --output_dir data/graphs --num_graphs 1000
python data_pipeline/init_edge_features.py --input_dir data/graphs --output_dir data/modified_graphs
```

### 2. Train the Model
Train your GNN model using the training script and configuration file:
```bash
python scripts/train_model.py --config experiments/default_config.yaml --data_dir data/modified_graphs --epochs 250
```

### 3. Hyperparameter Tuning
Optimize the modelâ€™s hyperparameters with Optuna:
```bash
python scripts/tune_hyperparams.py --n_trials 150 --study_name default --optuna_storage_path ./experiments/optuna_studies/study.db --min_resource 45 --max_resource 250 --reduction_factor 2
```

### 4. Run Inference
Import the inference script and use it to process a NetworkX graph and obtain node predictions, for example:
```python
import networkx as nx
from scripts import run_inference

# Create or load your NetworkX graph
nx_graph = nx.erdos_renyi_graph(100, 0.15)

# Ensure each node has the features expected by your model
for node in nx_graph.nodes():
    nx_graph.nodes[node]['feature'] = [0.5]  # Example feature

# Run inference to add the 'classification' attribute to nodes
result_graph = run_inference(
    nx_graph=nx_graph,
    model_path="experiments/trained_model.pt",
    device='auto'
)

# Retrieve predictions
predictions = nx.get_node_attributes(result_graph, 'classification')
print(predictions)
```

---

## ğŸ“Š Results & Visualization
Real-time experiment tracking and visualization are handled via **Weights & Biases (wandb)**.
Logging in to wandb is required to the training and hyperparameter tuning scripts.
---

## ğŸ“„ References
- **Shiran Gerassy-Vainberg & Shai S. Shen-Orr (2024)**, *A Personalized Network Framework Reveals Predictive Axis of Anti-TNF Response Across Diseases.*
- **Chung (1997)**, *Spectral Graph Theory*

### Evolution of GNNs leading to GAT:
- **Gori et al. (2005)**, *A new model for learning in graph domains.*  
  - One of the first formulations of **Graph Neural Networks (GNNs)**.  
- **Scarselli et al. (2008)**, *The Graph Neural Network Model.*  
  - Introduced a **formal recursive framework** for learning on graphs.  
- **Duvenaud et al. (2015)**, *Convolutional Networks on Graphs for Learning Molecular Fingerprints.*  
  - One of the first **graph convolutional approaches**, applied to molecular data.  
- **Vaswani et al. (2017)**, *Attention Is All You Need.* [[arXiv](https://arxiv.org/abs/1706.03762)]  
  - Introduced **Transformers**, which inspired attention-based mechanisms in GNNs like GAT and GATv2.
- **VelickoviÄ‡ et al. (2018)**, *Graph Attention Networks.* [[arXiv](https://arxiv.org/abs/1710.10903)]  
  - Proposed the original **Graph Attention Network (GAT)**, using attention mechanisms for adaptive neighborhood aggregation.
- **Brody, Alon & Yahav (2022)**, *How Attentive are Graph Attention Networks?* [[arXiv](https://arxiv.org/abs/2105.14491)]  
  - Introduced **GATv2**, an improved version of GAT with dynamic attention mechanisms.
- **Wu et al. (2019)**, *A Comprehensive Survey on Graph Neural Networks.*  
  - Summarizes the evolution of **GNN architectures**, including GCN, GraphSAGE, and GAT.  

---

## ğŸ¤ Contributors
- **Yaniv Slor Futterman** (yaniv.slor@campus.technion.ac.il)
- **Jonathan Israel** (jonathani@campus.technion.ac.il)

---

## ğŸ“œ License
This project is licensed under the Apache 2.0 License â€“ see the [`LICENSE`](LICENSE) file for details.

